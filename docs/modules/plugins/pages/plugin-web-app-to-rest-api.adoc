= Web Application & REST API Integration Plugin

The plugin provides the integration between web application testing functionality and REST API features.

:plugin-short-name: web-app-to-rest-api
include::partial$plugin-installation.adoc[]

== xref:ROOT:glossary.adoc#_table_transformer[Table Transformers]

=== FROM_SITEMAP

`FROM_SITEMAP` transformer generates table based on the website sitemap.

[cols="1,3", options="header"]
|===
|Parameter
|Description

|`siteMapRelativeUrl`
|relative URL of `sitemap.xml`

|`ignoreErrors`
|ignore sitemap parsing errors (_true_ or _false_)

|`column`
|the column name in the generated table
|===

[cols="3,1,1,3", options="header"]
|===
|Property Name
|Acceptable values
|Default
|Description

|`transformer.from-sitemap.ignore-errors`
a|`true`
`false`
|`false`
|ignore sitemap parsing errors

|`transformer.from-sitemap.filter-redirects`
a|`true`
`false`
|`false`
|defines whether urls that has redirect to the one that has already been included in the table are excluded from the table
|===
==== Required properties
* `web-application.main-page-url` - defines main application page URL

.Usage example
----
Examples:
{transformer=FROM_SITEMAP, siteMapRelativeUrl=/sitemap.xml, ignoreErrors=true, column=page-url}
----

=== FROM_HEADLESS_CRAWLING

`FROM_HEADLESS_CRAWLING` transformer generates table based on the results of headless crawling.

[cols="1,3", options="header"]
|===

|Parameter Name
|Description

|`column`
|The column name in the generated table.

|===

[cols="3,1,1,3", options="header"]
|===

|Property Name
|Acceptable values
|Default
|Description

4+^.^|_General_

|`transformer.from-headless-crawling.seed-relative-urls`
|Comma-separated list of values
|
|List of relative URLs, a seed URL is a URL that is fetched by the crawler to extract new URLs in it and follow them for crawling.

|`transformer.from-headless-crawling.exclude-extensions-regex`
|Regular expression
a |`(css\|gif\|gz\|ico\|jpeg\|jpg\|js\|mp3\|mp4\|pdf\|png\|svg\|zip\|woff2
\|woff\|ttf\|doc\|docx\|xml\|json\|webmanifest)`
|The regular expression to match extensions in URLs.
The crawler will ignore all URLs referring to files with extensions matching the given regular expression.
URLs without extensions will always be crawled.

|`transformer.from-headless-crawling.filter-redirects`
a|`true`
`false`
|`false`
|Defines whether urls that has redirect to the one that has already been included in the table are excluded from the table.

|`transformer.from-headless-crawling.socket-timeout`
|`integer`
|`40000`
|Socket timeout in milliseconds.

|`transformer.from-headless-crawling.connection-timeout`
|`integer`
|`30000`
|Connection timeout in milliseconds.

|`transformer.from-headless-crawling.max-download-size`
|`integer`
|`1048576`
|Max allowed size of a page in bytes. Pages larger than this size will not be fetched.

|`transformer.from-headless-crawling.max-connections-per-host`
|`integer`
|`100`
|Maximum connections per host.

|`transformer.from-headless-crawling.max-total-connections`
|`integer`
|`100`
|Maximum total connections.

|`transformer.from-headless-crawling.follow-redirects`
|`true` / `false`
|`true`
|Whether to follow redirects.

|`transformer.from-headless-crawling.max-depth-of-crawling`
|`integer`
|`-1`
|Maximum depth of crawling, for unlimited depth this parameter should be set to -1.

|`transformer.from-headless-crawling.max-pages-to-fetch`
|`integer`
|`-1`
|Number of pages to fetch, for unlimited number of pages this parameter should be set to -1.

|`transformer.from-headless-crawling.politeness-delay`
|`integer`
|`0`
|Politeness delay in milliseconds between sending two requests to the same host.

|`transformer.from-headless-crawling.max-outgoing-links-to-follow`
|`integer`
|`5000`
|Max number of outgoing links which are processed from a page.

|`transformer.from-headless-crawling.respect-no-follow`
a|`true`
`false`
|`false`
|Whether to honor links with https://en.wikipedia.org/wiki/Nofollow[nofollow flag].

|`transformer.from-headless-crawling.respect-no-index`
a|`true`
`false`
|`false`
|Whether to honor links with https://en.wikipedia.org/wiki/Noindex[noindex flag].

|`transformer.from-headless-crawling.user-agent-string`
|`string`
|`crawler4j (https://github.com/rzo1/crawler4j/)`
|https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent[User agent].

|`transformer.from-headless-crawling.cookie-policy`
|`ignore`, `standard`, `relaxed`
|`no default value`
|Cookie policy as defined per https://hc.apache.org/httpcomponents-client-4.5.x/current/tutorial/html/statemgmt.html#d5e515[cookie specification].

|`transformer.from-headless-crawling.allow-single-level-domain`
a|`true`
`false`
|`false`
|Whether to consider single level domains valid (e.g. http://localhost).

|`transformer.from-headless-crawling.include-https-pages`
a|`true`
`false`
|`true`
|Whether to crawl https pages.

4+^.^|_Proxy_

|`transformer.from-headless-crawling.proxy-host`
|`URL`
|`no default value`
|Proxy host.

|`transformer.from-headless-crawling.proxy-port`
|`integer`
|`80`
|Proxy port.

|`transformer.from-headless-crawling.proxy-username`
|`string`
|`no default value`
|Username to authenticate with proxy.

|`transformer.from-headless-crawling.proxy-password`
|`string`
|`no default value`
|Password to authenticate with proxy.

|===

==== Required properties

* `web-application.main-page-url` - defines main application page URL

.Usage example
----
Examples:
{transformer=FROM_HEADLESS_CRAWLING, column=page-url}
----

== Steps

=== Validate resources

Validates resources on web pages

Resource validation logic:

. If the `pages` row contains relative URL then it gets resolved against URL in `web-application.main-page-url` property, i.e. if the main page URL is `https://elderscrolls.bethesda.net/` and relative URL is `/skyrim10` the resulting URL will be `https://elderscrolls.bethesda.net/skyrim10`
. Collect elements by the CSS selector from each page
. Get either `href` or `src` attribute value from each element, if neither of the attributes exists the validation fails
. For each received value execute https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/HEAD[HEAD] request
.. If the status code is https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200[200 OK] then the resource validation is considered as passed
.. If the status code is one of https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404[404 Not Found], https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405[405 Method Not Allowed], https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/501[501 Not Implemented], https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/503[503 Service Unavailable] then GET request will be executed
.. If the https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/GET[GET] status code is https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200[200 OK] then the resource validation is considered as passed, otherwise failed

[source,gherkin]
----
Then all resources by selector `$cssSelector` are valid on:$pages
----

. `$cssSelector` - The https://www.w3schools.com/cssref/css_selectors.asp[CSS selector]
. `$pages` - The pages to validate resources on

.Validate resources
[source,gherkin]
----
Then all resources by selector `a` are valid on:
|pages                        |
|https://vividus.org/         |
|/test-automation-made-awesome|
----
