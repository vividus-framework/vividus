:main-page-url: xref:plugin-web-app.adoc#_properties[web-application.main-page-url]

= Web Application & REST API Integration Plugin

The plugin provides the integration between web application testing functionality and REST API features.

:plugin-short-name: web-app-to-rest-api
include::partial$plugin-installation.adoc[]

== xref:ROOT:glossary.adoc#_table_transformer[Table Transformers]

=== FROM_SITEMAP

`FROM_SITEMAP` transformer generates table based on the website sitemap.

[cols="1,3", options="header"]
|===
|Parameter
|Description

|`siteMapRelativeUrl`
|relative URL of `sitemap.xml`

|`ignoreErrors`
|ignore sitemap parsing errors (_true_ or _false_)

|`column`
|the column name in the generated table
|===

[cols="3,1,1,3", options="header"]
|===
|Property Name
|Acceptable values
|Default
|Description

|`transformer.from-sitemap.ignore-errors`
a|`true`
`false`
|`false`
|ignore sitemap parsing errors

|`transformer.from-sitemap.filter-redirects`
a|`true`
`false`
|`false`
|defines whether urls that has redirect to the one that has already been included in the table are excluded from the table
|===
==== Required properties
* `web-application.main-page-url` - defines main application page URL

.Usage example
----
Examples:
{transformer=FROM_SITEMAP, siteMapRelativeUrl=/sitemap.xml, ignoreErrors=true, column=page-url}
----

=== FROM_HEADLESS_CRAWLING

`FROM_HEADLESS_CRAWLING` transformer generates table based on the results of headless crawling.

[cols="1,3", options="header"]
|===

|Parameter Name
|Description

|`column`
|The column name in the generated table.

|===

[cols="3,1,1,3", options="header"]
|===

|Property Name
|Acceptable values
|Default
|Description

4+^.^|_General_

|`transformer.from-headless-crawling.seed-relative-urls`
|Comma-separated list of values
|
|List of relative URLs, a seed URL is a URL that is fetched by the crawler to extract new URLs in it and follow them for crawling.

|`transformer.from-headless-crawling.exclude-urls-regex`
|Regular expression
a |`.*(css\|gif\|gz\|ico\|jpeg\|jpg\|js\|mp3\|mp4\|pdf\|png\|svg\|zip\|woff2
\|woff\|ttf\|doc\|docx\|xml\|json\|webmanifest)$`
|The regular expression to match URLs.
The crawler will not crawl all URLs that matching the given regular expression and they will not be added to the resulting table.
https://en.wikipedia.org/wiki/URI_fragment[URI fragments] and https://en.wikipedia.org/wiki/Query_string[URL query] are ignored at filtering.

a|`transformer.from-headless-crawling.exclude-extensions-regex`
[WARNING]
====
The property is deprecated in favor of `transformer.from-headless-crawling.exclude-urls-regex` and will be removed in VIVIDUS 0.7.0.
====

|Regular expression
|no default value
|The regular expression to match extensions in URLs.
The crawler will ignore all URLs referring to files with extensions matching the given regular expression.

|`transformer.from-headless-crawling.filter-redirects`
a|`true`
`false`
|`false`
|Defines whether urls that has redirect to the one that has already been included in the table are excluded from the table.

|`transformer.from-headless-crawling.socket-timeout`
|`integer`
|`40000`
|Socket timeout in milliseconds.

|`transformer.from-headless-crawling.connection-timeout`
|`integer`
|`30000`
|Connection timeout in milliseconds.

|`transformer.from-headless-crawling.max-download-size`
|`integer`
|`1048576`
|Max allowed size of a page in bytes. Pages larger than this size will not be fetched.

|`transformer.from-headless-crawling.max-connections-per-host`
|`integer`
|`100`
|Maximum connections per host.

|`transformer.from-headless-crawling.max-total-connections`
|`integer`
|`100`
|Maximum total connections.

|`transformer.from-headless-crawling.follow-redirects`
|`true` / `false`
|`true`
|Whether to follow redirects.

|`transformer.from-headless-crawling.max-depth-of-crawling`
|`integer`
|`-1`
|Maximum depth of crawling, for unlimited depth this parameter should be set to -1.

|`transformer.from-headless-crawling.max-pages-to-fetch`
|`integer`
|`-1`
|Number of pages to fetch, for unlimited number of pages this parameter should be set to -1.

|`transformer.from-headless-crawling.politeness-delay`
|`integer`
|`0`
|Politeness delay in milliseconds between sending two requests to the same host.

|`transformer.from-headless-crawling.max-outgoing-links-to-follow`
|`integer`
|`5000`
|Max number of outgoing links which are processed from a page.

|`transformer.from-headless-crawling.respect-no-follow`
a|`true`
`false`
|`false`
|Whether to honor links with https://en.wikipedia.org/wiki/Nofollow[nofollow flag].

|`transformer.from-headless-crawling.respect-no-index`
a|`true`
`false`
|`false`
|Whether to honor links with https://en.wikipedia.org/wiki/Noindex[noindex flag].

|`transformer.from-headless-crawling.user-agent-string`
|`string`
|`crawler4j (https://github.com/rzo1/crawler4j/)`
|https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent[User agent].

|`transformer.from-headless-crawling.cookie-policy`
|`ignore`, `standard`, `relaxed`
|`no default value`
|Cookie policy as defined per https://hc.apache.org/httpcomponents-client-4.5.x/current/tutorial/html/statemgmt.html#d5e515[cookie specification].

|`transformer.from-headless-crawling.allow-single-level-domain`
a|`true`
`false`
|`false`
|Whether to consider single level domains valid (e.g. http://localhost).

|`transformer.from-headless-crawling.include-https-pages`
a|`true`
`false`
|`true`
|Whether to crawl https pages.

|`transformer.from-headless-crawling.http.headers.<header name>=<header value>`
|
|
a|Set of headers to set for every crawling request being sent.

[source,properties]
----
transformer.from-headless-crawling.http.headers.x-vercel-protection-bypass=1fac2b25014d35e5103b
----

4+^.^|_Proxy_

|`transformer.from-headless-crawling.proxy-host`
|`URL`
|`no default value`
|Proxy host.

|`transformer.from-headless-crawling.proxy-port`
|`integer`
|`80`
|Proxy port.

|`transformer.from-headless-crawling.proxy-username`
|`string`
|`no default value`
|Username to authenticate with proxy.

|`transformer.from-headless-crawling.proxy-password`
|`string`
|`no default value`
|Password to authenticate with proxy.

|===

==== Required properties

* `{main-page-url}` - defines main application page URL

.Usage example
----
Examples:
{transformer=FROM_HEADLESS_CRAWLING, column=page-url}
----

=== FROM_HTML

`FROM_HTML` transformer generates a table based on the text content, HTML content or attributes of HTML elements found in the requested HTML page.

[cols="1,3", options="header"]
|===

|Parameter Name
|Description

|`pageUrl`
a|The URL of the page to build the table upon.

WARNING: The `pageUrl` parameter is deprecated and will be removed in VIVIDUS 0.7.0, please use `variableName` instead.

IMPORTANT: The `pageUrl` parameter can not be used together with the `variableName` parameter.

|`variableName`
a|The name of the variable containing source HTML, only variables of scopes `global` and `next_batches` are allowed.
Exceptions are cases when the transformer using in xref:commons:vividus-steps.adoc#_initialize_variable_with_a_table[step]
which initializes a variable with a table.

IMPORTANT: The `variableName` parameter can not be used together with the `pageUrl` parameter.

|`column`
|The column name in the generated table.

|`xpathSelector`
a|The XPath selector to select HTML elements in the HTML page.

By using XPath selector we can extract element's HTML content, attributes and text content like its shown in the following example:

* `//a` - extract the link HTML content, e.g. `<a href="/hello">Say Hello</a>`
* `//a/text()` - extract the link text, e.g. `Say Hello`
* `//a/@href` - extract the link `href` attribute, e.g. `/hello`

|===

[cols="3,1,1,3", options="header"]
|===

|Property Name
|Acceptable values
|Default
|Description

|`transformer.from-html.headers.<header name>=<header value>`
|
|
a|Set of headers to set when requesting the page.

[source,properties]
----
transformer.from-html.headers.x-vercel-protection-bypass=1fac2b25014d35e5103b
----

|===

.Given the following HTML page
[source,html]
----
<!DOCTYPE html>
<html>
    <body>
        <a href="/r">R</a>
        <a href="/g">G</a>
        <a href="/b">B</a>
    </body>
</html>
----

.Applyng FROM_HTML to the page
----
Examples:
{transformer=FROM_HTML, column=relative-url, pageUrl=https://mypage.com, xpathSelector=//a/@href}
----

.Output table
----
|relative-url|
|/r          |
|/g          |
|/b          |
----

== Steps

=== Resources validations

:css-selector: https://www.w3schools.com/cssref/css_selectors.asp[CSS selector]

Steps to check resource availability using HTTP requests.

==== Resource validation statuses

[cols="1,4", options="header"]
|===
|Status
|Description

|`FAILED`
|An HTTP request to the resource returns a status code other than https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200[200 OK].

|`BROKEN`
a|Reasons:

* an HTTP request to the page under test returns an empty HTTP response body;
* an HTTP request to the page under test results in unexpected error;
* the relative page URL can not be resolved because the `{main-page-url}` property is not set;
* the resource has invalid URL format;
* the resource is missing `href` or `src` attributes;
* the resource has `href` or `src` attribute but its value is not a valid URL;
* the resource is a jump link that points to non-existent jump target.

|`PASSED`
|An HTTP request to the resource returns https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200[200 OK] status code.

|`FILTERED`
a|Reasons:

* the resource path matches the patterns specified by the `resource-checker.uri-to-ignore-regex` property;
* the resource path is equal to `#` (anchor);
* the resource is not a HTTP(S) resource.

|`SKIPPED`
|A resource validation has already been performed, i.e. if the same resource might be present on several pages so we do not need to validate it twice.

|===

==== Validate resources on web pages

Validates resources on web pages.

Resource validation logic:

. If the `pages` row contains relative URL then it gets resolved against URL in `{main-page-url}` property, i.e. if the main page URL is `https://elderscrolls.bethesda.net/` and relative URL is `/skyrim10` the resulting URL will be `https://elderscrolls.bethesda.net/skyrim10`
. Collect elements by the CSS selector from each page
. Get either `href` or `src` attribute value from each element, if neither of the attributes exists the validation fails
include::partial$resource-check-statuses.adoc[]

[source,gherkin]
----
Then all resources by selector `$cssSelector` are valid on:$pages
----

. `$cssSelector` - The {css-selector}.
. `$pages` - The pages to validate resources on.

.Validate resources
[source,gherkin]
----
Then all resources by selector `a` are valid on:
|pages                        |
|https://vividus.org/         |
|/test-automation-made-awesome|
----

==== Validate resources from HTML

Validates resources from HTML document.

Resource validation logic:

. Collects elements by the CSS selector from the specified HTML document
. Get either `href` or `src` attribute value from each element, if neither of the attributes exists the validation fails. If the element value contains relative URL then it gets resolved against URL in `{main-page-url}` property
include::partial$resource-check-statuses.adoc[]

[source,gherkin]
----
Then all resources by selector `$cssSelector` from $html are valid
----

. `$cssSelector` - {css-selector}.
. `$html` - HTML document to validate.

.Validate resources from the current page
[source,gherkin]
----
Then all resources by selector `a,img` from ${source-code} are valid
----

=== Validate redirects

Check that all URLs from xref:ROOT:glossary.adoc#_examplestable[ExamplesTable] redirect to proper pages with correct redirects number. Validation fails if either actual final URL or number of redirects do not match the expected values.

NOTE: The step throws the error in case if https://developer.mozilla.org/en-US/docs/Web/HTTP/Status[HTTP response status code] of checked URL out of range 200-207.

[source,gherkin]
----
Then I validate HTTP redirects: $expectedRedirects
----

. `$expectedRedirects` - The xref:ROOT:glossary.adoc#_examplestable[ExamplesTable] with redirect parameters containing the following columns:
** [subs=+quotes]`*startUrl*` - The URL from which redirection starts.
** [subs=+quotes]`*endUrl*` - The expected final URL to redirect to.
** [subs=+quotes]`*redirectsNumber*` - The expected number of redirects between `startUrl` and `endUrl` (optional).

.Validate redirects
[source,gherkin]
----
Then I validate HTTP redirects:
|startUrl                    |endUrl                          |redirectsNumber |
|http://example.com/redirect |http://example.com/get-response |1               |
----

=== Validate SSL rating

:grade: https://github.com/ssllabs/research/wiki/SSL-Server-Rating-Guide[grade]

Performs SSL scanning using https://www.ssllabs.com/index.html[SSL Labs] and compares received {grade} value with expected one.

[source,gherkin]
----
Then SSL rating for URL `$url` is $comparisonRule `$gradeName`
----

* `$url` - The URL for SSL scanning and grading.
* `$comparisonRule` - xref:parameters:comparison-rule.adoc[The comparison rule].
* `$gradeName` - The name of {grade}. The possible values: `A+`, `A`, `A-`, `B`, `C`, `D`, `E`, `F`, `T`, `M`.

.Properties
[cols="3,1,1,3", options="header"]
|===
|Property Name
|Acceptable values
|Default
|Description

|`ssl-labs.api-endpoint`
|`URL`
|`https://api.ssllabs.com`
|SSL Labs endpoint.

|===

.Validate SSL rating for `https://www.google.com`
[source,gherkin]
----
Then SSL rating for URL `https://www.google.com` is equal to `B`
----
